{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing ML functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import functions and helpers\n",
    "from helpers_higgs import *\n",
    "from implementations import *\n",
    "\n",
    "#import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load project data\n",
    "features, output, ids = load_training_data()\n",
    "y = output\n",
    "tx = build_model_data(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  3.14910656e-01  6.83319669e-02 ...  1.14381874e+00\n",
      "  -2.52714288e+00  4.12510497e-01]\n",
      " [ 1.00000000e+00  7.40827026e-01  5.52504823e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -2.73819964e-01]\n",
      " [ 1.00000000e+00 -5.38802302e-16  3.19515553e+00 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -2.93969845e-01]\n",
      " ...\n",
      " [ 1.00000000e+00 -3.10930673e-01  3.19316447e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -3.17017229e-01]\n",
      " [ 1.00000000e+00 -5.10097335e-01 -8.45323970e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -7.45439413e-01]\n",
      " [ 1.00000000e+00 -5.38802302e-16  6.65336083e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -7.45439413e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set up testing parameters\n",
    "max_iters = 100\n",
    "gamma = 0.05\n",
    "batch_size = 1\n",
    "lambda_ = 0.5\n",
    "w_initial = np.array([0] * 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD: execution time=1.1930540 seconds\n",
      "optimal weights:  [ 6.53440247e-01  9.43019160e-03  1.13615714e-01  4.65945620e-02\n",
      " -2.58835058e-02 -2.01017298e-02 -3.00760927e-02  4.64024550e-03\n",
      " -8.27969205e-02  1.57691394e-02 -1.72125969e-02  4.73445197e-02\n",
      " -6.17306924e-02 -3.69468591e-02 -7.44831100e-02  6.14892584e-04\n",
      "  9.64774016e-04 -5.42641163e-02  3.80266744e-04 -1.40143573e-03\n",
      " -2.04286092e-02 -8.17398890e-04  9.06117906e-03 -2.92136933e-03\n",
      "  1.22469706e-02 -1.56852877e-04 -3.30944738e-04  1.93453287e-02\n",
      " -8.10680826e-04  6.93801371e-04  8.92765351e-03]\n",
      "mse:  0.0864215304927601\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = mean_squared_error_gd(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: execution time=0.0144710 seconds\n",
      "optimal weights:  [ 0.68828651 -0.26052241  0.30180959 -0.10441406 -0.05549074  0.24686794\n",
      "  0.27661717 -0.20581963  0.17623275 -0.06896943 -0.10003348  0.17275089\n",
      "  0.13640004 -0.15780995 -0.1079856   0.26849156  0.08359543 -0.20827497\n",
      "  0.04179723  0.00239657  0.00898363 -0.08395189  0.01757371  0.07901825\n",
      " -0.15685631  0.04088467 -0.09843356 -0.09800659 -0.29140715 -0.0726017\n",
      " -0.04651181]\n",
      "mse:  0.5845334779905454\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = mean_squared_error_sgd(y, tx, w_initial, max_iters, gamma, batch_size)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS: execution time=0.0275140 seconds\n",
      "optimal weights:  [ 6.57332000e-01 -4.81728999e-03  1.27359614e-01  1.31751484e-01\n",
      "  5.50905807e-04 -1.09211915e-02 -4.50268911e-02 -2.41745297e-03\n",
      " -1.41004383e-01  1.40751290e-02  1.64629230e+02  9.40705754e-02\n",
      " -5.90325151e-02 -3.83086297e-02 -3.19811255e+01  3.89730267e-04\n",
      "  4.15328401e-04 -3.15390711e+01  4.30584336e-04 -1.25895666e-03\n",
      " -5.18296558e-02 -4.66893111e-04  2.35009511e-02 -2.08787977e-02\n",
      "  2.37891731e-02 -3.25363233e-04 -9.43779264e-05  1.83000923e-02\n",
      " -7.79186707e-04  8.71593305e-04 -1.39463298e+02]\n",
      "mse:  0.08510236304038934\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"LS: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for  0.0 :  0.08510236304081889\n",
      "mse for  0.00010101010101010101 :  0.08515663046517952\n",
      "mse for  0.00020202020202020202 :  0.08521054848401025\n",
      "mse for  0.00030303030303030303 :  0.08526442055254123\n",
      "mse for  0.00040404040404040404 :  0.0853182468533145\n",
      "mse for  0.000505050505050505 :  0.08537202756661261\n",
      "mse for  0.0006060606060606061 :  0.08542576287054306\n",
      "mse for  0.0007070707070707071 :  0.08547945294111443\n",
      "mse for  0.0008080808080808081 :  0.08553309795230743\n",
      "mse for  0.0009090909090909091 :  0.08558669807614099\n",
      "mse for  0.00101010101010101 :  0.08564025348273424\n",
      "mse for  0.0011111111111111111 :  0.08569376434036459\n",
      "mse for  0.0012121212121212121 :  0.0857472308155225\n",
      "mse for  0.0013131313131313131 :  0.0858006530729625\n",
      "mse for  0.0014141414141414141 :  0.08585403127575157\n",
      "mse for  0.0015151515151515152 :  0.0859073655853149\n",
      "mse for  0.0016161616161616162 :  0.08596065616147852\n",
      "mse for  0.0017171717171717172 :  0.08601390316251041\n",
      "mse for  0.0018181818181818182 :  0.08606710674515863\n",
      "mse for  0.0019191919191919192 :  0.08612026706468796\n",
      "mse for  0.00202020202020202 :  0.08617338427491443\n",
      "mse for  0.002121212121212121 :  0.08622645852823829\n",
      "mse for  0.0022222222222222222 :  0.08627948997567525\n",
      "mse for  0.0023232323232323234 :  0.0863324787668863\n",
      "mse for  0.0024242424242424242 :  0.086385425050206\n",
      "mse for  0.002525252525252525 :  0.08643832897266977\n",
      "mse for  0.0026262626262626263 :  0.08649119068003967\n",
      "mse for  0.0027272727272727275 :  0.08654401031682919\n",
      "mse for  0.0028282828282828283 :  0.08659678802632692\n",
      "mse for  0.002929292929292929 :  0.08664952395061945\n",
      "mse for  0.0030303030303030303 :  0.08670221823061308\n",
      "mse for  0.0031313131313131315 :  0.08675487100605468\n",
      "mse for  0.0032323232323232323 :  0.08680748241555208\n",
      "mse for  0.003333333333333333 :  0.08686005259659316\n",
      "mse for  0.0034343434343434343 :  0.08691258168556473\n",
      "mse for  0.0035353535353535356 :  0.08696506981777034\n",
      "mse for  0.0036363636363636364 :  0.08701751712744779\n",
      "mse for  0.003737373737373737 :  0.08706992374778583\n",
      "mse for  0.0038383838383838384 :  0.08712228981094029\n",
      "mse for  0.00393939393939394 :  0.0871746154480498\n",
      "mse for  0.00404040404040404 :  0.08722690078925092\n",
      "mse for  0.004141414141414141 :  0.08727914596369277\n",
      "mse for  0.004242424242424242 :  0.08733135109955131\n",
      "mse for  0.004343434343434344 :  0.08738351632404313\n",
      "mse for  0.0044444444444444444 :  0.08743564176343878\n",
      "mse for  0.004545454545454545 :  0.08748772754307574\n",
      "mse for  0.004646464646464647 :  0.08753977378737125\n",
      "mse for  0.004747474747474748 :  0.08759178061983433\n",
      "mse for  0.0048484848484848485 :  0.0876437481630779\n",
      "mse for  0.004949494949494949 :  0.08769567653883031\n",
      "mse for  0.00505050505050505 :  0.08774756586794677\n",
      "mse for  0.005151515151515152 :  0.08779941627042029\n",
      "mse for  0.0052525252525252525 :  0.08785122786539257\n",
      "mse for  0.005353535353535353 :  0.08790300077116434\n",
      "mse for  0.005454545454545455 :  0.08795473510520573\n",
      "mse for  0.005555555555555556 :  0.0880064309841663\n",
      "mse for  0.0056565656565656566 :  0.08805808852388462\n",
      "mse for  0.005757575757575757 :  0.08810970783939827\n",
      "mse for  0.005858585858585858 :  0.08816128904495259\n",
      "mse for  0.00595959595959596 :  0.08821283225401026\n",
      "mse for  0.006060606060606061 :  0.0882643375792602\n",
      "mse for  0.006161616161616161 :  0.088315805132626\n",
      "mse for  0.006262626262626263 :  0.08836723502527506\n",
      "mse for  0.006363636363636364 :  0.08841862736762644\n",
      "mse for  0.006464646464646465 :  0.08846998226935944\n",
      "mse for  0.0065656565656565654 :  0.08852129983942153\n",
      "mse for  0.006666666666666666 :  0.08857258018603639\n",
      "mse for  0.006767676767676768 :  0.08862382341671146\n",
      "mse for  0.006868686868686869 :  0.08867502963824578\n",
      "mse for  0.0069696969696969695 :  0.0887261989567374\n",
      "mse for  0.007070707070707071 :  0.08877733147759076\n",
      "mse for  0.007171717171717172 :  0.08882842730552375\n",
      "mse for  0.007272727272727273 :  0.08887948654457493\n",
      "mse for  0.0073737373737373735 :  0.08893050929811057\n",
      "mse for  0.007474747474747474 :  0.08898149566883139\n",
      "mse for  0.007575757575757576 :  0.08903244575877921\n",
      "mse for  0.007676767676767677 :  0.08908335966934376\n",
      "mse for  0.0077777777777777776 :  0.08913423750126902\n",
      "mse for  0.00787878787878788 :  0.08918507935465975\n",
      "mse for  0.00797979797979798 :  0.08923588532898773\n",
      "mse for  0.00808080808080808 :  0.08928665552309792\n",
      "mse for  0.008181818181818182 :  0.08933739003521464\n",
      "mse for  0.008282828282828282 :  0.08938808896294756\n",
      "mse for  0.008383838383838384 :  0.08943875240329766\n",
      "mse for  0.008484848484848484 :  0.0894893804526629\n",
      "mse for  0.008585858585858586 :  0.08953997320684412\n",
      "mse for  0.008686868686868687 :  0.08959053076105068\n",
      "mse for  0.008787878787878787 :  0.08964105320990601\n",
      "mse for  0.008888888888888889 :  0.089691540647453\n",
      "mse for  0.00898989898989899 :  0.08974199316715963\n",
      "mse for  0.00909090909090909 :  0.08979241086192406\n",
      "mse for  0.009191919191919192 :  0.0898427938240802\n",
      "mse for  0.009292929292929294 :  0.08989314214540256\n",
      "mse for  0.009393939393939394 :  0.08994345591711157\n",
      "mse for  0.009494949494949495 :  0.08999373522987872\n",
      "mse for  0.009595959595959595 :  0.09004398017383113\n",
      "mse for  0.009696969696969697 :  0.0900941908385571\n",
      "mse for  0.009797979797979799 :  0.09014436731311035\n",
      "mse for  0.009898989898989899 :  0.09019450968601515\n",
      "mse for  0.01 :  0.09024461804527094\n",
      "Ridge regression: execution time=2.6103470 seconds\n",
      "optimal weights:  [ 6.57332000e-01 -4.81714371e-03  1.27359415e-01  1.31751190e-01\n",
      "  5.49952663e-04 -1.09211545e-02 -4.50269347e-02 -2.41744809e-03\n",
      " -1.41004326e-01  1.40751121e-02  1.64628107e+02  9.40705696e-02\n",
      " -5.90324935e-02 -3.83086349e-02 -3.19809080e+01  3.89730371e-04\n",
      "  4.15328562e-04 -3.15388568e+01  4.30584329e-04 -1.25895665e-03\n",
      " -5.18296558e-02 -4.66893119e-04  2.35009510e-02 -2.08787977e-02\n",
      "  2.37891731e-02 -3.25363237e-04 -9.43779269e-05  1.83000923e-02\n",
      " -7.79186706e-04  8.71593292e-04 -1.39462347e+02]\n",
      "best loss:  0.08510236304081889\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "lambdas = np.linspace(0, 0.01, 100)\n",
    "wBest = []\n",
    "mseBest = 10000000\n",
    "for l in lambdas:\n",
    "    w, mse = ridge_regression(y, tx, l)\n",
    "    if mse < mseBest:\n",
    "        mseBest = mse\n",
    "        wBest = w\n",
    "    print(\"mse for \", l, \": \", mse)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", wBest)\n",
    "print(\"best loss: \",mseBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression: execution time=1.1906350 seconds\n",
      "optimal weights:  [ 6.53440247e-01  9.43019160e-03  1.13615714e-01  4.65945620e-02\n",
      " -2.58835058e-02 -2.01017298e-02 -3.00760927e-02  4.64024550e-03\n",
      " -8.27969205e-02  1.57691394e-02 -1.72125969e-02  4.73445197e-02\n",
      " -6.17306924e-02 -3.69468591e-02 -7.44831100e-02  6.14892584e-04\n",
      "  9.64774016e-04 -5.42641163e-02  3.80266744e-04 -1.40143573e-03\n",
      " -2.04286092e-02 -8.17398890e-04  9.06117906e-03 -2.92136933e-03\n",
      "  1.22469706e-02 -1.56852877e-04 -3.30944738e-04  1.93453287e-02\n",
      " -8.10680826e-04  6.93801371e-04  8.92765351e-03]\n",
      "log loss:  0.0864215304927601\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, loss = mean_squared_error_gd(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"logistic regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"log loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for  0.0 :  0.5417921911916318\n",
      "loss for  0.00010101010101010101 :  0.5439091043055989\n",
      "loss for  0.00020202020202020202 :  0.5454478643561357\n",
      "loss for  0.00030303030303030303 :  0.5412506342901685\n",
      "loss for  0.00040404040404040404 :  0.5417784058926014\n",
      "loss for  0.000505050505050505 :  0.5451229589212915\n",
      "loss for  0.0006060606060606061 :  0.544603028621449\n",
      "loss for  0.0007070707070707071 :  0.5427137204596663\n",
      "loss for  0.0008080808080808081 :  0.5465234897238507\n",
      "loss for  0.0009090909090909091 :  0.5421815931177373\n",
      "loss for  0.00101010101010101 :  0.5480273590207223\n",
      "loss for  0.0011111111111111111 :  0.5455919440206517\n",
      "loss for  0.0012121212121212121 :  0.543672955489049\n",
      "loss for  0.0013131313131313131 :  0.5420397241988203\n",
      "loss for  0.0014141414141414141 :  0.5444750067741524\n",
      "loss for  0.0015151515151515152 :  0.5454664104412279\n",
      "loss for  0.0016161616161616162 :  0.5491336134351281\n",
      "loss for  0.0017171717171717172 :  0.5441551306454809\n",
      "loss for  0.0018181818181818182 :  0.5455392925987033\n",
      "loss for  0.0019191919191919192 :  0.547882785505654\n",
      "loss for  0.00202020202020202 :  0.5465643993535044\n",
      "loss for  0.002121212121212121 :  0.5449762576053838\n",
      "loss for  0.0022222222222222222 :  0.5461075819256913\n",
      "loss for  0.0023232323232323234 :  0.5444373974229474\n",
      "loss for  0.0024242424242424242 :  0.5458608541780584\n",
      "loss for  0.002525252525252525 :  0.5452279930890405\n",
      "loss for  0.0026262626262626263 :  0.5451115985391842\n",
      "loss for  0.0027272727272727275 :  0.5488106924891831\n",
      "loss for  0.0028282828282828283 :  0.5475463087515405\n",
      "loss for  0.002929292929292929 :  0.5469316732946751\n",
      "loss for  0.0030303030303030303 :  0.544622146482505\n",
      "loss for  0.0031313131313131315 :  0.5441670714693985\n",
      "loss for  0.0032323232323232323 :  0.5442787012184478\n",
      "loss for  0.003333333333333333 :  0.5482333817082512\n",
      "loss for  0.0034343434343434343 :  0.5549815970643791\n",
      "loss for  0.0035353535353535356 :  0.5466797570316301\n",
      "loss for  0.0036363636363636364 :  0.5475739973405417\n",
      "loss for  0.003737373737373737 :  0.5462533756801901\n",
      "loss for  0.0038383838383838384 :  0.5471157892105072\n",
      "loss for  0.00393939393939394 :  0.5456761145247744\n",
      "loss for  0.00404040404040404 :  0.5481883730781909\n",
      "loss for  0.004141414141414141 :  0.5441747756703704\n",
      "loss for  0.004242424242424242 :  0.5462556679042868\n",
      "loss for  0.004343434343434344 :  0.5435535409606829\n",
      "loss for  0.0044444444444444444 :  0.5465214571408024\n",
      "loss for  0.004545454545454545 :  0.547862966831815\n",
      "loss for  0.004646464646464647 :  0.5459515068108397\n",
      "loss for  0.004747474747474748 :  0.5494368638861219\n",
      "loss for  0.0048484848484848485 :  0.5492031589171491\n",
      "loss for  0.004949494949494949 :  0.5466409856062713\n",
      "loss for  0.00505050505050505 :  0.5488577675863263\n",
      "loss for  0.005151515151515152 :  0.5477747141211283\n",
      "loss for  0.0052525252525252525 :  0.5486264944682813\n",
      "loss for  0.005353535353535353 :  0.5466449028113736\n",
      "loss for  0.005454545454545455 :  0.5499720873536089\n",
      "loss for  0.005555555555555556 :  0.5462599663917649\n",
      "loss for  0.0056565656565656566 :  0.5512401257119599\n",
      "loss for  0.005757575757575757 :  0.5475151037146951\n",
      "loss for  0.005858585858585858 :  0.545235260313825\n",
      "loss for  0.00595959595959596 :  0.551038317464465\n",
      "loss for  0.006060606060606061 :  0.5467677837334092\n",
      "loss for  0.006161616161616161 :  0.5479298733644538\n",
      "loss for  0.006262626262626263 :  0.5510360123828982\n",
      "loss for  0.006363636363636364 :  0.547800401737374\n",
      "loss for  0.006464646464646465 :  0.549346507283121\n",
      "loss for  0.0065656565656565654 :  0.5458132000985126\n",
      "loss for  0.006666666666666666 :  0.5479108102816462\n",
      "loss for  0.006767676767676768 :  0.5495780980106808\n",
      "loss for  0.006868686868686869 :  0.5468818163963515\n",
      "loss for  0.0069696969696969695 :  0.5508948745104247\n",
      "loss for  0.007070707070707071 :  0.5522971796222569\n",
      "loss for  0.007171717171717172 :  0.5475122598189754\n",
      "loss for  0.007272727272727273 :  0.5516583480804453\n",
      "loss for  0.0073737373737373735 :  0.5506738956263045\n",
      "loss for  0.007474747474747474 :  0.5514320774488142\n",
      "loss for  0.007575757575757576 :  0.5503785538386189\n",
      "loss for  0.007676767676767677 :  0.5491620987025433\n",
      "loss for  0.0077777777777777776 :  0.5491275624892294\n",
      "loss for  0.00787878787878788 :  0.5509551497175647\n",
      "loss for  0.00797979797979798 :  0.5491852381991682\n",
      "loss for  0.00808080808080808 :  0.5528122359265927\n",
      "loss for  0.008181818181818182 :  0.5516037141986313\n",
      "loss for  0.008282828282828282 :  0.5499712394048352\n",
      "loss for  0.008383838383838384 :  0.548701015696459\n",
      "loss for  0.008484848484848484 :  0.5508152806485754\n",
      "loss for  0.008585858585858586 :  0.5500891091751489\n",
      "loss for  0.008686868686868687 :  0.5508982867215458\n",
      "loss for  0.008787878787878787 :  0.5474829957599828\n",
      "loss for  0.008888888888888889 :  0.5522546172481826\n",
      "loss for  0.00898989898989899 :  0.548168610452306\n",
      "loss for  0.00909090909090909 :  0.55068947471819\n",
      "loss for  0.009191919191919192 :  0.5512233267836057\n",
      "loss for  0.009292929292929294 :  0.5487806406903134\n",
      "loss for  0.009393939393939394 :  0.5529355869625299\n",
      "loss for  0.009494949494949495 :  0.5523992521744807\n",
      "loss for  0.009595959595959595 :  0.550970658578421\n",
      "loss for  0.009696969696969697 :  0.5528546748020905\n",
      "loss for  0.009797979797979799 :  0.5502282933384637\n",
      "loss for  0.009898989898989899 :  0.5511384713093921\n",
      "loss for  0.01 :  0.5493964590177168\n",
      "regularized logistic regression: execution time=3.0092940 seconds\n",
      "optimal weights:  [ 4.88383897e-01 -7.94231557e-03  4.11983503e-01  3.45317131e-02\n",
      " -1.17909127e-01 -7.52929704e-02 -8.70058008e-02  6.75995580e-02\n",
      " -1.08047675e-01  9.07861981e-02 -5.13616485e-02  2.08959155e-01\n",
      " -2.57770420e-01 -1.32863623e-01 -2.06354854e-01  1.61031831e-02\n",
      "  2.29218087e-02  4.83118827e-03 -1.41342749e-02 -3.67404937e-02\n",
      "  6.62470521e-03  3.53064400e-03 -2.32850382e-02 -3.48260317e-02\n",
      "  1.23983871e-05 -7.56292586e-03 -1.88228085e-02  8.23421937e-02\n",
      "  1.17079037e-02 -4.53043091e-02 -1.45344573e-02]\n",
      "best loss:  0.5412506342901685\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "lambdas = np.linspace(0, 0.01, 100)\n",
    "wBest = []\n",
    "bestLoss = 10000000\n",
    "for l in lambdas:\n",
    "    w, loss = reg_logistic_regression(y, tx, l, w_initial, max_iters, gamma)\n",
    "    if loss < bestLoss:\n",
    "        bestLoss = loss\n",
    "        wBest = w\n",
    "    print(\"loss for \", l, \": \", loss)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "# loss = log loss + regularizer loss\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"regularized logistic regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", wBest)\n",
    "print(\"best loss: \", bestLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
