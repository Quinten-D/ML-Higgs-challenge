{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing ML functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import functions and helpers\n",
    "from helpers_higgs import *\n",
    "from implementations import *\n",
    "\n",
    "#import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load project data\n",
    "features, output, ids = load_training_data()\n",
    "y = output\n",
    "tx = build_model_data(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  3.14910656e-01  6.83319669e-02 ...  1.14381874e+00\n",
      "  -2.52714288e+00  4.12510497e-01]\n",
      " [ 1.00000000e+00  7.40827026e-01  5.52504823e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -2.73819964e-01]\n",
      " [ 1.00000000e+00 -5.38802302e-16  3.19515553e+00 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -2.93969845e-01]\n",
      " ...\n",
      " [ 1.00000000e+00 -3.10930673e-01  3.19316447e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -3.17017229e-01]\n",
      " [ 1.00000000e+00 -5.10097335e-01 -8.45323970e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -7.45439413e-01]\n",
      " [ 1.00000000e+00 -5.38802302e-16  6.65336083e-01 ... -1.74353029e-17\n",
      "  -1.68378328e-17 -7.45439413e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set up testing parameters\n",
    "max_iters = 100\n",
    "gamma = 0.05\n",
    "batch_size = 1\n",
    "lambda_ = 0.5\n",
    "w_initial = np.array([0] * 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD: execution time=1.1930540 seconds\n",
      "optimal weights:  [ 6.53440247e-01  9.43019160e-03  1.13615714e-01  4.65945620e-02\n",
      " -2.58835058e-02 -2.01017298e-02 -3.00760927e-02  4.64024550e-03\n",
      " -8.27969205e-02  1.57691394e-02 -1.72125969e-02  4.73445197e-02\n",
      " -6.17306924e-02 -3.69468591e-02 -7.44831100e-02  6.14892584e-04\n",
      "  9.64774016e-04 -5.42641163e-02  3.80266744e-04 -1.40143573e-03\n",
      " -2.04286092e-02 -8.17398890e-04  9.06117906e-03 -2.92136933e-03\n",
      "  1.22469706e-02 -1.56852877e-04 -3.30944738e-04  1.93453287e-02\n",
      " -8.10680826e-04  6.93801371e-04  8.92765351e-03]\n",
      "mse:  0.0864215304927601\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = mean_squared_error_gd(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: execution time=0.0144710 seconds\n",
      "optimal weights:  [ 0.68828651 -0.26052241  0.30180959 -0.10441406 -0.05549074  0.24686794\n",
      "  0.27661717 -0.20581963  0.17623275 -0.06896943 -0.10003348  0.17275089\n",
      "  0.13640004 -0.15780995 -0.1079856   0.26849156  0.08359543 -0.20827497\n",
      "  0.04179723  0.00239657  0.00898363 -0.08395189  0.01757371  0.07901825\n",
      " -0.15685631  0.04088467 -0.09843356 -0.09800659 -0.29140715 -0.0726017\n",
      " -0.04651181]\n",
      "mse:  0.5845334779905454\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = mean_squared_error_sgd(y, tx, w_initial, max_iters, gamma, batch_size)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## least_squares_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS: execution time=0.0275140 seconds\n",
      "optimal weights:  [ 6.57332000e-01 -4.81728999e-03  1.27359614e-01  1.31751484e-01\n",
      "  5.50905807e-04 -1.09211915e-02 -4.50268911e-02 -2.41745297e-03\n",
      " -1.41004383e-01  1.40751290e-02  1.64629230e+02  9.40705754e-02\n",
      " -5.90325151e-02 -3.83086297e-02 -3.19811255e+01  3.89730267e-04\n",
      "  4.15328401e-04 -3.15390711e+01  4.30584336e-04 -1.25895666e-03\n",
      " -5.18296558e-02 -4.66893111e-04  2.35009511e-02 -2.08787977e-02\n",
      "  2.37891731e-02 -3.25363233e-04 -9.43779264e-05  1.83000923e-02\n",
      " -7.79186707e-04  8.71593305e-04 -1.39463298e+02]\n",
      "mse:  0.08510236304038934\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, mse = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"LS: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for  0.0 :  0.08510236304081889\n",
      "mse for  0.00010101010101010101 :  0.08515663046517952\n",
      "mse for  0.00020202020202020202 :  0.08521054848401025\n",
      "mse for  0.00030303030303030303 :  0.08526442055254123\n",
      "mse for  0.00040404040404040404 :  0.0853182468533145\n",
      "mse for  0.000505050505050505 :  0.08537202756661261\n",
      "mse for  0.0006060606060606061 :  0.08542576287054306\n",
      "mse for  0.0007070707070707071 :  0.08547945294111443\n",
      "mse for  0.0008080808080808081 :  0.08553309795230743\n",
      "mse for  0.0009090909090909091 :  0.08558669807614099\n",
      "mse for  0.00101010101010101 :  0.08564025348273424\n",
      "mse for  0.0011111111111111111 :  0.08569376434036459\n",
      "mse for  0.0012121212121212121 :  0.0857472308155225\n",
      "mse for  0.0013131313131313131 :  0.0858006530729625\n",
      "mse for  0.0014141414141414141 :  0.08585403127575157\n",
      "mse for  0.0015151515151515152 :  0.0859073655853149\n",
      "mse for  0.0016161616161616162 :  0.08596065616147852\n",
      "mse for  0.0017171717171717172 :  0.08601390316251041\n",
      "mse for  0.0018181818181818182 :  0.08606710674515863\n",
      "mse for  0.0019191919191919192 :  0.08612026706468796\n",
      "mse for  0.00202020202020202 :  0.08617338427491443\n",
      "mse for  0.002121212121212121 :  0.08622645852823829\n",
      "mse for  0.0022222222222222222 :  0.08627948997567525\n",
      "mse for  0.0023232323232323234 :  0.0863324787668863\n",
      "mse for  0.0024242424242424242 :  0.086385425050206\n",
      "mse for  0.002525252525252525 :  0.08643832897266977\n",
      "mse for  0.0026262626262626263 :  0.08649119068003967\n",
      "mse for  0.0027272727272727275 :  0.08654401031682919\n",
      "mse for  0.0028282828282828283 :  0.08659678802632692\n",
      "mse for  0.002929292929292929 :  0.08664952395061945\n",
      "mse for  0.0030303030303030303 :  0.08670221823061308\n",
      "mse for  0.0031313131313131315 :  0.08675487100605468\n",
      "mse for  0.0032323232323232323 :  0.08680748241555208\n",
      "mse for  0.003333333333333333 :  0.08686005259659316\n",
      "mse for  0.0034343434343434343 :  0.08691258168556473\n",
      "mse for  0.0035353535353535356 :  0.08696506981777034\n",
      "mse for  0.0036363636363636364 :  0.08701751712744779\n",
      "mse for  0.003737373737373737 :  0.08706992374778583\n",
      "mse for  0.0038383838383838384 :  0.08712228981094029\n",
      "mse for  0.00393939393939394 :  0.0871746154480498\n",
      "mse for  0.00404040404040404 :  0.08722690078925092\n",
      "mse for  0.004141414141414141 :  0.08727914596369277\n",
      "mse for  0.004242424242424242 :  0.08733135109955131\n",
      "mse for  0.004343434343434344 :  0.08738351632404313\n",
      "mse for  0.0044444444444444444 :  0.08743564176343878\n",
      "mse for  0.004545454545454545 :  0.08748772754307574\n",
      "mse for  0.004646464646464647 :  0.08753977378737125\n",
      "mse for  0.004747474747474748 :  0.08759178061983433\n",
      "mse for  0.0048484848484848485 :  0.0876437481630779\n",
      "mse for  0.004949494949494949 :  0.08769567653883031\n",
      "mse for  0.00505050505050505 :  0.08774756586794677\n",
      "mse for  0.005151515151515152 :  0.08779941627042029\n",
      "mse for  0.0052525252525252525 :  0.08785122786539257\n",
      "mse for  0.005353535353535353 :  0.08790300077116434\n",
      "mse for  0.005454545454545455 :  0.08795473510520573\n",
      "mse for  0.005555555555555556 :  0.0880064309841663\n",
      "mse for  0.0056565656565656566 :  0.08805808852388462\n",
      "mse for  0.005757575757575757 :  0.08810970783939827\n",
      "mse for  0.005858585858585858 :  0.08816128904495259\n",
      "mse for  0.00595959595959596 :  0.08821283225401026\n",
      "mse for  0.006060606060606061 :  0.0882643375792602\n",
      "mse for  0.006161616161616161 :  0.088315805132626\n",
      "mse for  0.006262626262626263 :  0.08836723502527506\n",
      "mse for  0.006363636363636364 :  0.08841862736762644\n",
      "mse for  0.006464646464646465 :  0.08846998226935944\n",
      "mse for  0.0065656565656565654 :  0.08852129983942153\n",
      "mse for  0.006666666666666666 :  0.08857258018603639\n",
      "mse for  0.006767676767676768 :  0.08862382341671146\n",
      "mse for  0.006868686868686869 :  0.08867502963824578\n",
      "mse for  0.0069696969696969695 :  0.0887261989567374\n",
      "mse for  0.007070707070707071 :  0.08877733147759076\n",
      "mse for  0.007171717171717172 :  0.08882842730552375\n",
      "mse for  0.007272727272727273 :  0.08887948654457493\n",
      "mse for  0.0073737373737373735 :  0.08893050929811057\n",
      "mse for  0.007474747474747474 :  0.08898149566883139\n",
      "mse for  0.007575757575757576 :  0.08903244575877921\n",
      "mse for  0.007676767676767677 :  0.08908335966934376\n",
      "mse for  0.0077777777777777776 :  0.08913423750126902\n",
      "mse for  0.00787878787878788 :  0.08918507935465975\n",
      "mse for  0.00797979797979798 :  0.08923588532898773\n",
      "mse for  0.00808080808080808 :  0.08928665552309792\n",
      "mse for  0.008181818181818182 :  0.08933739003521464\n",
      "mse for  0.008282828282828282 :  0.08938808896294756\n",
      "mse for  0.008383838383838384 :  0.08943875240329766\n",
      "mse for  0.008484848484848484 :  0.0894893804526629\n",
      "mse for  0.008585858585858586 :  0.08953997320684412\n",
      "mse for  0.008686868686868687 :  0.08959053076105068\n",
      "mse for  0.008787878787878787 :  0.08964105320990601\n",
      "mse for  0.008888888888888889 :  0.089691540647453\n",
      "mse for  0.00898989898989899 :  0.08974199316715963\n",
      "mse for  0.00909090909090909 :  0.08979241086192406\n",
      "mse for  0.009191919191919192 :  0.0898427938240802\n",
      "mse for  0.009292929292929294 :  0.08989314214540256\n",
      "mse for  0.009393939393939394 :  0.08994345591711157\n",
      "mse for  0.009494949494949495 :  0.08999373522987872\n",
      "mse for  0.009595959595959595 :  0.09004398017383113\n",
      "mse for  0.009696969696969697 :  0.0900941908385571\n",
      "mse for  0.009797979797979799 :  0.09014436731311035\n",
      "mse for  0.009898989898989899 :  0.09019450968601515\n",
      "mse for  0.01 :  0.09024461804527094\n",
      "Ridge regression: execution time=2.6103470 seconds\n",
      "optimal weights:  [ 6.57332000e-01 -4.81714371e-03  1.27359415e-01  1.31751190e-01\n",
      "  5.49952663e-04 -1.09211545e-02 -4.50269347e-02 -2.41744809e-03\n",
      " -1.41004326e-01  1.40751121e-02  1.64628107e+02  9.40705696e-02\n",
      " -5.90324935e-02 -3.83086349e-02 -3.19809080e+01  3.89730371e-04\n",
      "  4.15328562e-04 -3.15388568e+01  4.30584329e-04 -1.25895665e-03\n",
      " -5.18296558e-02 -4.66893119e-04  2.35009510e-02 -2.08787977e-02\n",
      "  2.37891731e-02 -3.25363237e-04 -9.43779269e-05  1.83000923e-02\n",
      " -7.79186706e-04  8.71593292e-04 -1.39462347e+02]\n",
      "best loss:  0.08510236304081889\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "lambdas = np.linspace(0, 0.01, 100)\n",
    "wBest = []\n",
    "mseBest = 10000000\n",
    "for l in lambdas:\n",
    "    w, mse = ridge_regression(y, tx, l)\n",
    "    if mse < mseBest:\n",
    "        mseBest = mse\n",
    "        wBest = w\n",
    "    print(\"mse for \", l, \": \", mse)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", wBest)\n",
    "print(\"best loss: \",mseBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression: execution time=0.0351260 seconds\n",
      "optimal weights:  [ 0.47320573 -0.01070983  0.42319858  0.02242195 -0.09923217 -0.09988554\n",
      " -0.12958563  0.06603944 -0.16182909  0.06654029 -0.04505975  0.1708772\n",
      " -0.2222028  -0.17016316 -0.19944772 -0.00347226  0.01238146 -0.00413056\n",
      "  0.01497756  0.0369873   0.06599184 -0.01371485 -0.01229633 -0.01822273\n",
      " -0.01430562  0.03258326 -0.00327953  0.0663124   0.02162453 -0.00752552\n",
      " -0.00665713]\n",
      "log loss:  0.540910960601009\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "w, loss = logistic_regression(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"logistic regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", w)\n",
    "print(\"log loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for  0.0 :  0.5446808397716106\n",
      "loss for  0.00010101010101010101 :  0.5426465432038703\n",
      "loss for  0.00020202020202020202 :  0.5432937050463691\n",
      "loss for  0.00030303030303030303 :  0.5423473435268725\n",
      "loss for  0.00040404040404040404 :  0.5428660153283377\n",
      "loss for  0.000505050505050505 :  0.5452126508641172\n",
      "loss for  0.0006060606060606061 :  0.5458403319517211\n",
      "loss for  0.0007070707070707071 :  0.543848906046724\n",
      "loss for  0.0008080808080808081 :  0.54678265860054\n",
      "loss for  0.0009090909090909091 :  0.5433616520435771\n",
      "loss for  0.00101010101010101 :  0.5476435815552677\n",
      "loss for  0.0011111111111111111 :  0.5437694185007143\n",
      "loss for  0.0012121212121212121 :  0.5453719379738634\n",
      "loss for  0.0013131313131313131 :  0.5441009520621815\n",
      "loss for  0.0014141414141414141 :  0.5460141850781061\n",
      "loss for  0.0015151515151515152 :  0.5456316298896744\n",
      "loss for  0.0016161616161616162 :  0.5461819944740229\n",
      "loss for  0.0017171717171717172 :  0.5422602113731322\n",
      "loss for  0.0018181818181818182 :  0.5445541596205128\n",
      "loss for  0.0019191919191919192 :  0.5439263717228928\n",
      "loss for  0.00202020202020202 :  0.548440295546865\n",
      "loss for  0.002121212121212121 :  0.5476270460261702\n",
      "loss for  0.0022222222222222222 :  0.5443043934490374\n",
      "loss for  0.0023232323232323234 :  0.5442610845453991\n",
      "loss for  0.0024242424242424242 :  0.5453662627520485\n",
      "loss for  0.002525252525252525 :  0.5446842020322359\n",
      "loss for  0.0026262626262626263 :  0.5449343045148026\n",
      "loss for  0.0027272727272727275 :  0.5440417978163115\n",
      "loss for  0.0028282828282828283 :  0.5448445729725453\n",
      "loss for  0.002929292929292929 :  0.5487403917601444\n",
      "loss for  0.0030303030303030303 :  0.5510431536655828\n",
      "loss for  0.0031313131313131315 :  0.5516051422867044\n",
      "loss for  0.0032323232323232323 :  0.5509728988830808\n",
      "loss for  0.003333333333333333 :  0.5462126397430158\n",
      "loss for  0.0034343434343434343 :  0.5459550765789808\n",
      "loss for  0.0035353535353535356 :  0.5443727238453248\n",
      "loss for  0.0036363636363636364 :  0.5480842801774145\n",
      "loss for  0.003737373737373737 :  0.5460766135151371\n",
      "loss for  0.0038383838383838384 :  0.5459449500724993\n",
      "loss for  0.00393939393939394 :  0.5491873917549682\n",
      "loss for  0.00404040404040404 :  0.5436669709938152\n",
      "loss for  0.004141414141414141 :  0.548304720468389\n",
      "loss for  0.004242424242424242 :  0.5461712706395874\n",
      "loss for  0.004343434343434344 :  0.5464497765634336\n",
      "loss for  0.0044444444444444444 :  0.5482096853062485\n",
      "loss for  0.004545454545454545 :  0.5465707189515984\n",
      "loss for  0.004646464646464647 :  0.5464693658708518\n",
      "loss for  0.004747474747474748 :  0.5505816058420396\n",
      "loss for  0.0048484848484848485 :  0.5495604877205311\n",
      "loss for  0.004949494949494949 :  0.5478575037751383\n",
      "loss for  0.00505050505050505 :  0.5466718808179478\n",
      "loss for  0.005151515151515152 :  0.547861324625818\n",
      "loss for  0.0052525252525252525 :  0.5492245417511272\n",
      "loss for  0.005353535353535353 :  0.5495316481146549\n",
      "loss for  0.005454545454545455 :  0.5482893264996489\n",
      "loss for  0.005555555555555556 :  0.548915735802127\n",
      "loss for  0.0056565656565656566 :  0.5498583279099389\n",
      "loss for  0.005757575757575757 :  0.5488779584774092\n",
      "loss for  0.005858585858585858 :  0.5461262127215178\n",
      "loss for  0.00595959595959596 :  0.5520071809056764\n",
      "loss for  0.006060606060606061 :  0.5482722831994126\n",
      "loss for  0.006161616161616161 :  0.5511515047635515\n",
      "loss for  0.006262626262626263 :  0.5464641975534323\n",
      "loss for  0.006363636363636364 :  0.5491469754528041\n",
      "loss for  0.006464646464646465 :  0.5488957855711383\n",
      "loss for  0.0065656565656565654 :  0.551017757280047\n",
      "loss for  0.006666666666666666 :  0.5470089284062126\n",
      "loss for  0.006767676767676768 :  0.5482792225329762\n",
      "loss for  0.006868686868686869 :  0.5476477084380941\n",
      "loss for  0.0069696969696969695 :  0.5488683374066312\n",
      "loss for  0.007070707070707071 :  0.5483994022484922\n",
      "loss for  0.007171717171717172 :  0.5499454280376985\n",
      "loss for  0.007272727272727273 :  0.5512155201526284\n",
      "loss for  0.0073737373737373735 :  0.5487035679002776\n",
      "loss for  0.007474747474747474 :  0.5523194348574815\n",
      "loss for  0.007575757575757576 :  0.5509067597920224\n",
      "loss for  0.007676767676767677 :  0.5475514884681779\n",
      "loss for  0.0077777777777777776 :  0.5491789083721732\n",
      "loss for  0.00787878787878788 :  0.5498562149998248\n",
      "loss for  0.00797979797979798 :  0.5515382207284885\n",
      "loss for  0.00808080808080808 :  0.5511146960084661\n",
      "loss for  0.008181818181818182 :  0.549409579029576\n",
      "loss for  0.008282828282828282 :  0.5485896145601847\n",
      "loss for  0.008383838383838384 :  0.5486062091516444\n",
      "loss for  0.008484848484848484 :  0.5507725220334764\n",
      "loss for  0.008585858585858586 :  0.5513776526281615\n",
      "loss for  0.008686868686868687 :  0.5506820544543702\n",
      "loss for  0.008787878787878787 :  0.552598402419251\n",
      "loss for  0.008888888888888889 :  0.5483702545559822\n",
      "loss for  0.00898989898989899 :  0.5521184548713649\n",
      "loss for  0.00909090909090909 :  0.54917187001493\n",
      "loss for  0.009191919191919192 :  0.5493086218031354\n",
      "loss for  0.009292929292929294 :  0.5518039438551734\n",
      "loss for  0.009393939393939394 :  0.5526461681324941\n",
      "loss for  0.009494949494949495 :  0.5514870841116297\n",
      "loss for  0.009595959595959595 :  0.5524307764187237\n",
      "loss for  0.009696969696969697 :  0.5503248352667802\n",
      "loss for  0.009797979797979799 :  0.5503438299114881\n",
      "loss for  0.009898989898989899 :  0.5517591931234047\n",
      "loss for  0.01 :  0.5515936801691903\n",
      "regularized logistic regression: execution time=2.7757800 seconds\n",
      "optimal weights:  [ 4.70756124e-01  1.20220579e-03  4.13125652e-01  3.42056189e-02\n",
      " -1.01339843e-01 -1.10036921e-01 -1.16117621e-01  5.76145803e-02\n",
      " -1.55341327e-01  9.57956751e-02 -5.60899603e-02  2.01055655e-01\n",
      " -2.07578215e-01 -1.73290677e-01 -2.20237873e-01  7.03668742e-03\n",
      " -1.89289195e-03  7.29846975e-03  1.65960876e-02 -3.75415997e-02\n",
      "  4.56751355e-02 -4.34446522e-02 -2.75897053e-02 -2.92630471e-02\n",
      " -1.54125617e-02  3.12965960e-02  3.69147751e-02  5.07936379e-02\n",
      "  1.92410401e-04 -4.08134892e-02 -1.74971257e-02]\n",
      "best loss:  0.5422602113731322\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "lambdas = np.linspace(0, 0.01, 100)\n",
    "wBest = []\n",
    "bestLoss = 10000000\n",
    "for l in lambdas:\n",
    "    w, loss = reg_logistic_regression(y, tx, l, w_initial, max_iters, gamma)\n",
    "    if loss < bestLoss:\n",
    "        bestLoss = loss\n",
    "        wBest = w\n",
    "    print(\"loss for \", l, \": \", loss)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "# loss = log loss + regularizer loss\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"regularized logistic regression: execution time={t:.7f} seconds\".format(t=exection_time))\n",
    "print(\"optimal weights: \", wBest)\n",
    "print(\"best loss: \", bestLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
